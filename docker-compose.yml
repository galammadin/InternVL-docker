services:
  qolda-lmdeploy:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: qolda-lmdeploy
    ports:
      - "23333:23333"
    environment:
      - MODEL_NAME=issai/Qolda
      - SERVER_PORT=23333
      - BACKEND=pytorch
      - TP=1
      - SESSION_LEN=32768
    volumes:
      # Cache HuggingFace models to avoid re-downloading
      - huggingface-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '8gb'

volumes:
  huggingface-cache:
